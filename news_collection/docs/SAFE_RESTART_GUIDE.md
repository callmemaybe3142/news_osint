# Safe Restart Guide - No Duplicates!

## âœ… **Short Answer: Just Stop and Restart!**

**You can safely:**
1. Stop the current collector
2. Update the code
3. Restart the collector

**No duplicates will be created!**

---

## ğŸ›¡ï¸ **Why It's Safe**

### **1. Database Primary Key Protection**

```sql
-- Messages table has composite primary key
PRIMARY KEY (id)
UNIQUE (channel_id, message_id)

-- PostgreSQL prevents duplicates automatically!
INSERT INTO messages (channel_id, message_id, ...)
VALUES (1234567890, 20054, ...)
ON CONFLICT (channel_id, message_id) DO NOTHING;
-- â†‘ If message exists, it's skipped!
```

**Result:**
- âœ… Same message can't be inserted twice
- âœ… Database enforces uniqueness
- âœ… No duplicate messages

---

### **2. Image Deduplication**

```sql
-- Images table has unique file_id
file_id TEXT NOT NULL UNIQUE

INSERT INTO images (file_id, ...)
VALUES ('abc123', ...)
ON CONFLICT (file_id) DO NOTHING;
-- â†‘ If image exists, it's skipped!
```

**Result:**
- âœ… Same image can't be inserted twice
- âœ… File system check prevents re-download
- âœ… No duplicate images

---

### **3. Last Fetched Tracking**

```python
# Collector remembers where it left off
last_fetched = channel_info.get('last_fetched_datetime')

if last_fetched:
    # Only fetch messages AFTER last fetch
    offset_date = last_fetched.astimezone(timezone.utc)
    logger.info(f"Fetching messages since: {offset_date}")
else:
    # First run - start from START_DATE
    offset_date = self.config.START_DATE
```

**Result:**
- âœ… Only fetches NEW messages
- âœ… Skips already collected messages
- âœ… Efficient incremental updates

---

## ğŸ”„ **What Happens When You Restart**

### **Scenario 1: Restart During Collection**

**Current state:**
```
Channel 1: Collected 5,000 messages (last: 2025-12-20 08:00)
Channel 2: Collected 3,000 messages (last: 2025-12-20 07:30)
Channel 3: Not started yet
```

**After restart:**
```
Channel 1: Starts from 2025-12-20 08:00 (continues where it left off)
Channel 2: Starts from 2025-12-20 07:30 (continues where it left off)
Channel 3: Starts from beginning (as planned)
```

**Result:**
- âœ… No duplicates
- âœ… Continues from last position
- âœ… Only fetches new messages

---

### **Scenario 2: Restart After Completion**

**Current state:**
```
All channels: Fully collected up to 2025-12-20 09:00
```

**After restart:**
```
All channels: Fetch messages from 2025-12-20 09:00 to now
```

**Result:**
- âœ… Only new messages collected
- âœ… No re-downloading old messages
- âœ… Efficient incremental update

---

## ğŸ“‹ **Safe Restart Procedure**

### **Step 1: Stop Current Collector**

**If using systemd:**
```bash
sudo systemctl stop news-collector
```

**If using screen:**
```bash
# Attach to screen
screen -r collector

# Stop with Ctrl+C
# Detach with Ctrl+A, D
```

**If using nohup:**
```bash
# Find process
ps aux | grep collector.py

# Kill process
kill <PID>
```

---

### **Step 2: Update Code**

```bash
cd /var/www/news_osint/news_collection

# Pull latest changes or edit files
nano image_handler.py
nano collector.py
nano config.py
```

**Changes you made:**
- âœ… `image_handler.py` - Async optimization
- âœ… `collector.py` - Concurrent channels (5)
- âœ… `config.py` - Batch size 500

---

### **Step 3: Restart Collector**

**If using systemd:**
```bash
sudo systemctl start news-collector
sudo journalctl -u news-collector -f
```

**If using screen:**
```bash
screen -S collector
cd /var/www/news_osint/news_collection
python3 collector.py
# Ctrl+A, D to detach
```

---

## ğŸ” **Verify No Duplicates**

### **Check Database**

```sql
-- Count total messages
SELECT COUNT(*) FROM messages;

-- Check for duplicate (channel_id, message_id) pairs
SELECT channel_id, message_id, COUNT(*) 
FROM messages 
GROUP BY channel_id, message_id 
HAVING COUNT(*) > 1;
-- Should return 0 rows!

-- Check for duplicate images
SELECT file_id, COUNT(*) 
FROM images 
GROUP BY file_id 
HAVING COUNT(*) > 1;
-- Should return 0 rows!
```

---

## âš ï¸ **What Gets Re-processed**

### **Messages:**
- âŒ **NOT re-downloaded** (skipped by `ON CONFLICT`)
- âŒ **NOT re-inserted** (database prevents it)
- âœ… **Only new messages** fetched

### **Images:**
- âŒ **NOT re-downloaded** (file exists check)
- âŒ **NOT re-inserted** (unique file_id)
- âœ… **Only new images** downloaded

### **Database Queries:**
- âœ… Duplicate check still runs (fast with index)
- âœ… Text hash comparison (for new messages)
- âœ… No wasted processing

---

## ğŸ“Š **Performance Impact of Restart**

### **During First Run (Old Code):**
```
70,000 messages in 12 hours
```

### **After Restart (New Code):**
```
Only NEW messages since last run
Example: 1,000 new messages in 10 minutes
```

**Why so fast?**
- âœ… Only fetches messages after `last_fetched_datetime`
- âœ… Skips already collected messages
- âœ… Database prevents duplicate inserts
- âœ… New async code is 3-6x faster

---

## ğŸ¯ **Best Practices**

### **1. Stop Gracefully**

```bash
# Don't kill -9 (force kill)
kill -9 <PID>  # âŒ Bad - may corrupt data

# Use normal kill or Ctrl+C
kill <PID>     # âœ… Good - graceful shutdown
# or Ctrl+C    # âœ… Good - graceful shutdown
```

### **2. Wait for Current Batch**

```
# Collector will finish current batch before stopping
Processing batch: channel1, channel2, channel3, channel4
^C (Ctrl+C pressed)
Finishing current batch...
Database connection pool closed
```

### **3. Check Logs After Restart**

```bash
# Verify it's working
sudo journalctl -u news-collector -f

# Look for:
"Fetching messages since: 2025-12-20 08:00"  # âœ… Good
"Processing 5 channels concurrently"          # âœ… New code working
"Saved image: ..."                            # âœ… Async working
```

---

## ğŸš¨ **When You WOULD Get Duplicates**

### **Scenario: Manual Database Deletion**

```sql
-- If you manually delete last_fetched_datetime
UPDATE channels SET last_fetched_datetime = NULL;
-- âŒ Collector will re-fetch all messages!
-- But database will still prevent duplicates (ON CONFLICT)
```

**Result:**
- âš ï¸ Re-downloads all messages (waste of time)
- âœ… But no duplicates (database prevents it)
- âš ï¸ Waste of bandwidth and time

**Don't do this unless you want to re-collect everything!**

---

## âœ… **Summary**

### **Can I Restart Safely?**
**YES!** âœ…

### **Will I Get Duplicates?**
**NO!** âœ…

### **Why?**
1. âœ… Database primary key prevents duplicate messages
2. âœ… Unique file_id prevents duplicate images
3. âœ… `last_fetched_datetime` tracks progress
4. âœ… `ON CONFLICT DO NOTHING` skips existing records

### **What Should I Do?**
```bash
# 1. Stop collector
sudo systemctl stop news-collector

# 2. Update code (already done!)
# - image_handler.py (async)
# - collector.py (concurrent 5)
# - config.py (batch 500)

# 3. Restart collector
sudo systemctl start news-collector

# 4. Monitor logs
sudo journalctl -u news-collector -f

# Done! No duplicates, much faster!
```

### **Expected Result:**
- âœ… Continues from where it left off
- âœ… Only fetches new messages
- âœ… 3-6x faster with new code
- âœ… No duplicates
- âœ… No data loss

**You're safe to restart anytime!** ğŸš€
